Multivariate statistics
  1. Introduction
    1.1 Goals of multivariate statistical technique
      Graph to show benefit of using multivariate stats
        Create 95% confidence interval
          Both mean rent and housing prices
        Rectangle vs ellipsoid
          Marginal confidence interval
          Bivariate confidence ellipsoid
    1.2 Data reduction or structural simplification
      Matrix scatterplot
        Shows how all pairs of variables are correlated
        Remove redundat variable
    1.3 Grouping and classifying observations
      Should pluto be classified as a planet or like other kuiper belt object
      How do cancers group together
      How are southern and western state different
    1.4 Examination of dependence among variables
      Fastfood menu and calorie and nutrition distribution
      health index
    1.5 Describing relationships between groups of variables
      Housing cost, population, income
    1.6 Hypothesis formulation and testing
      Hypothesis generated from the data
    1.7 Multivariate graphics and distributions
    1.8 why R?
    1.9 Additional Readings
  2. Elements of R
    2.1 Getting started in R
      2.1.1 R as a calculator
      2.1.2 Vectors in R
        3 %in% (1:5) => TRUE
        match(3:4, 2:12) =>2,3
        which (x > 2)
      2.1.3 Printing in R
        cat (c("x is", x))
    2.2 Simulation and simple statistics
      Generate observations from standard normal distribution (0,1)
        rnorm(3)
      Continous statistical distributions
        Normal
        Uniform
        Student's t
        chisquared (central and non-central)
        Gamma
        Multivariate normal
        Multivariate student's t
        Exponential
        Cauchy
        Wishart
      Function list
        Density function
        cumulative distribution
        Quantile function
        General random variable
    2.3 Handling datasets
      read.table()
      R also treats the data frame as matrix
    2.4 Basic data manipulation and statistics
      Some functions
        sort
        order
        colMeans
        var
        cor
        sapply
        merge
        duplicated
    2.5 Programming and writing functions in R
    2.6 A larger simulation
      Correlation coefficient
      simcor ??
    2.7 Advanced numerical operations
      nlm
    2.8 Housekeeping
    2.9 Exercises
  3. Graphical displays
    3.1 Graphics in R
    3.2 Displays for Univariate data
      Scatterplots
      boxplots
      histogram
    3.3 Displays for Bivariate data
      3.3.1 Plot options, colors and characters
      3.3.2 More graphics for bivariate data
        Bivariate box plot
        Inner one 50% of data
        Outer 95% of data
      Convex hull
        smallest convex polygon with all the data
        identifies extremes in data
        successive layers
        inner layer dense
    3.4 Displays for Three-dimensional data
        Bubble plor of altitude for lat, long
        Kriging
          weather like plots
    3.5 Displays for Higher dimensional data
      3.5.1 Pairs, Bagplot and Coplot
        Basic matrix like scatterplot
        scatterplot with loess smoothing
      3.5.2 Glyphs: Stars and Faces
        bagplot
          connection two convex hulls
        Burger coplot
        Star plot
        Chernoff's faces
      3.5.3 Parallel coordinates
    3.6 Additional reading
    3.7 Exercises
  4. Basic linear algebra
    4.1 Apples and oranges
      Scalar
      Vector
      Matrix
    4.2 Vectors
      Transpose of vector
      Vector multiplied by scalar constant c
      sum of two vectors, x+y = y+x
      vector addition
        valid only when same length
        commutative
      Scalar-valued inner product
        Operation only defined by vectors of similar length
        x'y= ∑xiyi
        x'y = y'x
        linear operator
          x'(y+z) = x'y + x'z
        linearly dependent
          if x1=cx2 for any scalar c
        Linear dependence for a set of vectors
          c1x1 + c2x2 + cnxn = 0 
          scalars c1,c2,cn not all zero
          Implication of linear dependence
            At least one vector can be expressed as linear combination of others
        Mutually independent vectors
          They should not be dependent
          c1=c2==cn=0 
          Mutually orthogonal vectors are independent
          Linearly independent vectors need not be orthogonal
        Euclidean length
          square root of the inner product with itself (Slide1)
    4.3 Basic Matrix Arithmetic
      Transpose of a matrix
        Reverse subscripts
      symmetrix matrix
        Matrix = transpose of matrix
        Only square matrix can be symmetric
      Dimension for matrix addition
        Two matrices with same dimension can be added
      matrix multiplication
        Dimension = Z(nxp) = X(nxm) * Y(mxp)
        Matrix multiplication is not commutative
      Square diagonal matrix
        zero entries off the main diagonal
      Identity matrix
        Diagonal matrix with 1 in diagonal
        same effect as multiplying by 1 
        I (nxn) X(nxm) = X(nxm) I(mxm) = X (nxm)
    4.4 Matrix operations in R
      Some arithmetic
        Addition of two vectors
          x+y
        Multiplication and products
          inner product
            Output not a scalar (1x1 matrix)
            two vectors
              crossprod(x,y)
            vector with itself
              crossprod(x) = x'x
          multiplication
            x*y
            component wise product of vectors
            Not the vector cross product
          Outer product
            xx' is nxn matrix
            x %o% x
            To get 3rd row
              (x %o% x)[3,]
            To get 2nd column
              (x %o% x)[,2]
          Multiplication of two matrices
            A %*% B
            Product of 3x4 and 4x3 matrix is a 3x3 matrix
        To get diagonal of a matrix
          diag(1,4,4)
            all non-diag zero
            4x4 identity matrix
        To get all 1 4x4 matrix
          matrix (1,4,4)
        To get a 4x4 1,16 running down column
          matrix (1:16, 4,4)
        To get a 4x4 1,16 running down rows
          t(matrix(1:16,4,4)
        diag function
          Build diagonal matrices
          extract diagonal of a matrix
          diag(t(matrix(1:16,4,4)))
      Solve linear equations
        Ax=b
        solve(A,b) (Slide1)
        x = A-1b where A-1 is the inverse of A
          A-1 can be obtained by using solve(A)
    4.5 Advanced matrix operations
      4.5.1 Determinants
        Trace of a square matrix
          Sum of its diagonal elements
        Determinant
          rowise matrix = a b, c d  
            ad -bc
          rowwise matrix = abc, def, ghi  
            a (ei-fh) + b (fg -id) + c(dh -eg)
          Determinant is a polynomial in matrix elements
            Grows quickly in complexity
          Formula det(x)
        Special determinant properties
          Determinant of a diagonal matrix
            Product of diagonal entries
          Determinant of identity matrix
            1 regardless of size
          Determinant of a matrix = Determinant of transpose
            Det(A) = Det(A')
          Determinant of scalar c multiple of nxn matrix
            Det(cA) = c^n Det(A)
          Determinant of a product of square matrices
            Det(AB) = Det(A)Det(B)
        Rank of a matrix
          Maximum number of linearly independent rows and columns
        Singularity
          non-singular
            Full rank
            rank is equal to number of rows (or columns)
          singular
            Square matrix that is less than full rank
            has determinant of zero
      4.5.2 Matrix inversion
        Inverse of a matrix
          May or may not exist
          properties
            A-1A = AA-1 = I = Identity matrix
            equivalent to reciprocal of scalar
            Inverse of I is I
            Matrix with zero determinant has no inverse
          Calculation of inverse
            For2x2
              A=ab, cd
              A-1 = 1/det(A) (d-b, -ca)
            For 3x3 (slide1)
            Inverse only exists when determinant is non-zero
            in R
              A-1 = solve(A)
        Multiply matrix by its inverse in R
            x %*% solve(x) = I (with numerical errors)
        Relationship between matrix determinant and inverse
          Det(A-1) = 1/Det(A)
        Inverse of a matrix not always needed
          Shortcut exists to avoid some scenarios
            b'A-1b = b%*% solve(A,b)
            No need to separately find A-1
      4.5.3 Eigenvalues and eigenvectors
        Property of square matrix
        Every square matrix has a set of eigenvalues and eigenvectors
        Linear equation
          Ax =λx 
            A = square matrix
            λ = eigenvalue
            x = eigenvector
          Intuition
            effect of multiplying vector x by A is same as multiplying by λ 
            λ is scalar
              Multiplication by scalar changes
                stretching
                shrinking
                changing direction
            If one, then infinite solution
              (x,λ ) and (2x, λ/2)
              To avoid confusion Find set of eigenvectors 
                of unit length 
                and mutually orthogonal
          Ix = x
            λs of identity matrix are all equal to 1
            Every vector x is an eigenvector of identity matrix
        Some useful properties
          Trace(A) = λ1 + λ2 + λn
          Det(A) = λ1λ2..λn
          Characteristic polynomial formula
            p(λ) = Det(A-λI)
            Roots of the equation p(λ)=0 determine eigen values
            R uses diff algorithm
        Roots and λ
          nxn matrix will have n eigenvalues
          some eigenvalues may be repeated 
            like multiple roots of polynomial
          Complex numbers
            Polynomial may have complex roots
            Appear as complex conjugates (slide1)
        Calculation of eigenvalue in R
          evs <- eigen(A) for symmetric matrix (A)
            evs$val
            evs$vec
          eigen(A, only.values=TRUE)$values
          eigen(x)$values
        prod same as det
          prod(eigen(x)$values)
          det(x)
        Definite square matrix
          Positive definite square matrix
            For every non-zero vector x -- x'Ax > 0
            All eigen values are real and non-negtive
          Non-negative definite
            x'Ax >=0
      4.5.4 Diagonalizable matrices
        Square matrix A diagnoaliizabe if
          A=XDX-1
        eigen function
          Numerical identification of components of diagnolizable matrix
          ?? go back to text
      4.5.5 Generalized inverses
        Also known as Moore-penrose invese
        AA-A = A
          When it exists has a similar property when it doesnt
        Not unique
        ??go back to text
      4.5.6 Matrix square root
        ??go back to text
    4.6 Exercises
  5. The Univariate normal distribution
    Development of normal distribution
      Moivre
        Approximation to binomical distribution in 1738
      Gauss
        1809
        Gaussian distribution
        bell curve
          Familiar shape of density function
    5.1 The normal density and distribution functions
      standard normal distribution
        density function is phi(x) (slide2)
        R-dnorm()
        called standard normal because random variable has 0 mean and 1 unit variance
        What happens if we change mean but same unit variance
          shifts left or right with same shape
        linear transformation of X
          X - standard mean 0 variance 1
            z = μ + σX
            z will also be normally distributed with mean mu and variance σ^2
        Density function for distribution z = μ + σX
          phi (slide2)
        Effect of change of μ
          left and right shift
          plot shape remains same
        Effect of change of σ
          plot becomes thinner and taller or wide and flatter
      R functions
        def
          x,q: vector of quantiles
          p: vector of probabilities
          n: number of observation
          mu: vector of means
          sd: vector of standard deviations
        def2
          
          
        CDF 
          F(x) = P(X <= x)   
        dnorm (x, mu, sd)
          density
          d for "density", the density function (p. f. or p. d. f.)
          Gets a normal version from some set of random numbers
        pnorm (q, mu, sd)
          distribution function
          p for "probability", the cumulative distribution function (c. d. f.)
          factory produces weights; what is the probability x <=19
          what is p (x>19)
            1-(x<=19)
        qnorm (p, mu, sd)
          quantile function
          q for "quantile", the inverse c. d. f.
          p = F(x)  ;   x = F-1(p) 
          Given p, identify p-th quantile of normal distribution
          IQ has mean 100, sd 15, what is 95 percentile?
          for qnorm use .05/2 to get half og probability
          qnorm(0.95, mean=100, sd=15)
        rnorm (n, mu, sd)
          random variates
          r for "random", a random variable having the specified distribution
          random samples from standard normal distribution
        
        Uniform distribution
        dunif(x, min = 0, max = 1, log = FALSE)
        punif(q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
        qunif(p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
        runif(n, min = 0, max = 1)
    5.2 Relationship to other distributions
      Normal approximation of binomial distribution
      Close relationship between normal and chi-squared distribution
        if x is normal, 
          x^2 behaves as chi-squared with 1 df
        if x1,x2... xk each independent as standard normals
          x1^2 + ... xk^2 behaves as chi-squared with kdf
            also known as cauchy distribution
        sums and differences of correlated variates also behave as normal
    5.3 Transformations to Normality
      Issues
        use non-parametric if not normal
        binary valued data 
      Non-linear transformation
        Power transformation
          Box and cox
        Power (x)
          -x^lambda for lambda < 0
            reciprocals of large number are smaller
            so negative sign maintains the order
          log(x) for lambda = 0
          x^lambda for lambda >0
      Effects
        Further lambda from 1 greater effect of transformation
        lambda < 1 
          pulling long right tail and spreading in short left tail
        lamba > 1
          pulling left tail
        Histogram of median apartment rent (slide 3)
          For lambda, 
            1=same
            1/2= 1/2 exponent or root
              seems best
              bimodal problem can not be solved
            0=log
            -1/2 exponent
            -1 reciprocal
            -1.5 exponent
        How to choose value of lamdba
          art than science
          apply separate statistical test
      What are the expected values from an ordered normal sample
        all n observation equal cumulative probabilities between each (slide3)
        spread out evenly
        Expected quantiles for qq plot
          formula for xihat (slide 3) 
          qq plot is scatterplot of  xihat , xi 
            ideally should be straight line
            qqplot(y)
            qqline(y)
      Evaluation of qqplot of rent data
        quartiles divided the observed data into four equal sized groups
        1/4 below first quartile
        1/4 above 3rd quartiles
        line connects first and 3rd quartile
        original
          lower tail (left) is short to where normal distribution would expect it
          lower tail is not as spread
          Upper tail looks ok
          center shows bump indicative of bimodal
        Transformed 1/2
          both left and right tail slightly shorter than expected
          center shows bump indicative of bimodal
    5.4 Tests for Normality
    5.5 Inference on Univariate normal means
    5.6 Inference on variances
    5.7 Maximum likelihood estimation part I
    5.8 Exercises
  6. Bivariate normal distribution
    6.1 The bivariate normal density function
    6.2 Properties of the bivariate normal distribution
    6.3 Inference on bivariate normal parameters
    6.4 Tests for bivariate normality
    6.5 Maximum likelihood estimation, part II
    6.6 Exercises
  7. Multivariate normal distribution
    7.1 Multivariate normal density and its properties
    7.2 Inference on multivariate normal means
    7.3 Example: Home price index
    7.4 Maximum likelihood, part III: models for means
    7.5 Inference on multivariate normal variances
    7.6 Fitting patterned covariance matrices
    7.7 Tests for multivariate normality
    7.8 Exercises
  8. Factor methods
    8.1 Principal components analysis
    8.2 Example 1:Investment allocations
    8.3 Example 2: Kuiper Belt objects
    8.4 Example 3: Health outcomes in US Hospitals
    8.5 Factor analysis
    8.6 Exercises
  9. Multivariable linear regression
    9.1 Univariate regression
    9.2 Multivariable regression in R
    9.3 A large health survey
    9.4 Exercies
  10. Discriminants and classification
    10.1 An introductory example
    10.2 Multinomial logistic regression
    10.3 Linear discriminant analysis
    10.4 Support vector machine
    10.5 Regression trees
    10.6 Exercises
  11. Clustering
    11.1 Hierarchical clustering
    11.2 K-means clustering
    11.3 Diagnosis, validation, and other methods
    11.4 Exercises
  12. Time series models
    12.1 Introductory examples and simple analyses
    12.2 Autoregressive models
    12.3 Spectral decomposition
    12.4 Exercises
  13. Other useful methods
    13.1 Ranking from paired comparisons
    13.2 Canonical correlations
    13.3 Methods for extreme order statistics
    13.4 Big data and wide data
    13.5 Exercises
  content
    https://www.safaribooksonline.com/library/view/data-mining-and/9781118868706/
    https://www.youtube.com/watch?v=eNQTnPXNIFA&list=PLbMVogVj5nJRt-ZxRG1KRjxNoy7J_IaW2
  14. Appendix
